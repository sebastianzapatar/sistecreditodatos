{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bec7f48-476f-4080-915c-80b676b7fc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === LEER ARCHIVO .env DE FORMA SEGURA ===\n",
    "\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Ruta correcta (sin /dbfs al inicio para load_dotenv)\n",
    "env_path = \"/dbfs/Users/pansezapata@gmail.com/.env\"\n",
    "\n",
    "# Cargar variables desde .env\n",
    "result = load_dotenv(env_path)\n",
    "print(f\".env cargado: {result}\")\n",
    "\n",
    "# Leer variables de entorno\n",
    "storage_account = os.getenv('AZURE_STORAGE_ACCOUNT')\n",
    "storage_key = os.getenv('AZURE_STORAGE_KEY')\n",
    "\n",
    "print(f\"Storage Account: {storage_account}\")\n",
    "print(f\"Storage Key: {'*' * 12} (protegida)\")\n",
    "\n",
    "# Verificar que no sean None\n",
    "if storage_account and storage_key:\n",
    "    # Configurar Spark\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "        storage_key\n",
    "    )\n",
    "    print(\"Configuraci√≥n Spark completada!\")\n",
    "else:\n",
    "    print(\"Error: Variables de entorno est√°n vac√≠as\")\n",
    "    \n",
    "print(\"prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dda0671-7b9c-4c5a-8d0d-0278374859c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Leer datos (usando el m√©todo h√≠brido que ya funciona)\n",
    "print(\"üìñ Cargando datos...\")\n",
    "file_path = \"abfss://sistecredito2@sistecreditofinal.dfs.core.windows.net/data/v1/train/credit_train.csv\"\n",
    "spark_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "df = spark_df.toPandas()\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(\"Columnas:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6d8824-cbac-4b87-af54-bf00c3285339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explore_credit_data(df):\n",
    "    \"\"\"An√°lisis exploratorio espec√≠fico para datos de cr√©dito\"\"\"\n",
    "    \n",
    "    print(\"üîç === AN√ÅLISIS EXPLORATORIO ===\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nInformaci√≥n del dataset:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Valores nulos\n",
    "    print(\"\\nValores nulos por columna:\")\n",
    "    nulls = df.isnull().sum()\n",
    "    if nulls.sum() > 0:\n",
    "        print(nulls[nulls > 0])\n",
    "    else:\n",
    "        print(\"No hay valores nulos\")\n",
    "    \n",
    "    # Distribuci√≥n de la variable objetivo (asumiendo que existe)\n",
    "    target_cols = [col for col in df.columns if 'default' in col.lower() or 'risk' in col.lower() or 'target' in col.lower()]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"\\nDistribuci√≥n de la variable objetivo '{target_col}':\")\n",
    "        print(df[target_col].value_counts())\n",
    "        print(\"Porcentaje:\")\n",
    "        print(df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return target_col if target_cols else None\n",
    "\n",
    "# Ejecutar EDA\n",
    "target_column = explore_credit_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ce4081-d477-45fc-b978-05041886e921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba82f268-9717-4f44-810d-eb48e38f53f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_credit_data(df, target_col):\n",
    "    \"\"\"Preprocesar datos para el modelo\"\"\"\n",
    "    \n",
    "    print(\" === PREPROCESAMIENTO ===\")\n",
    "    \n",
    "    # Crear copia para no modificar original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Limpiar datos\n",
    "    print(\"Limpiando datos...\")\n",
    "    df_processed = df_processed.dropna()\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    \n",
    "    print(f\"‚úÖ Datos despu√©s de limpieza: {df_processed.shape}\")\n",
    "    \n",
    "    # Identificar tipos de columnas\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remover target de las listas si est√° presente\n",
    "    if target_col:\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "        if target_col in categorical_cols:\n",
    "            categorical_cols.remove(target_col)\n",
    "    \n",
    "    print(f\"Columnas num√©ricas ({len(numeric_cols)}): {numeric_cols}\")\n",
    "    print(f\"Columnas categ√≥ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col != target_col:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Preparar variable objetivo\n",
    "    if target_col and target_col in categorical_cols:\n",
    "        le_target = LabelEncoder()\n",
    "        df_processed[target_col] = le_target.fit_transform(df_processed[target_col].astype(str))\n",
    "        label_encoders[target_col] = le_target\n",
    "    \n",
    "    return df_processed, numeric_cols, categorical_cols, label_encoders\n",
    "\n",
    "# Ejecutar preprocessing\n",
    "df_processed, num_cols, cat_cols, encoders = preprocess_credit_data(df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9689f296-000c-4306-9cfa-9d2be1a260b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest_model(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Entrenar modelo Random Forest con validaci√≥n robusta de tipos de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"ü§ñ === ENTRENAMIENTO RANDOM FOREST ===\")\n",
    "    \n",
    "    # 1. VALIDAR INPUTS\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"‚ùå Target column '{target_col}' no encontrada en el dataset\")\n",
    "    \n",
    "    missing_features = [col for col in feature_cols if col not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"‚ùå Features no encontradas: {missing_features}\")\n",
    "    \n",
    "    # 2. PREPARAR DATOS CON VALIDACI√ìN DE TIPOS\n",
    "    print(\"üìä Preparando datos...\")\n",
    "    \n",
    "    # Crear copias y verificar tipos\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    print(f\"üìä Shape inicial - X: {X.shape}, y: {y.shape}\")\n",
    "    print(f\"üéØ Target: '{target_col}'\")\n",
    "    print(f\"üìä Tipo de y: {y.dtype}\")\n",
    "    \n",
    "    # 3. LIMPIAR Y VALIDAR TARGET\n",
    "    print(\"üîç Validando target...\")\n",
    "    \n",
    "    # Verificar valores nulos en target\n",
    "    if y.isnull().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è Target tiene {y.isnull().sum()} valores nulos - eliminando...\")\n",
    "        valid_indices = ~y.isnull()\n",
    "        X = X[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "    \n",
    "    # Verificar tipo del target\n",
    "    print(f\"üìä Valores √∫nicos en target: {y.unique()}\")\n",
    "    \n",
    "    # Asegurar que el target sea num√©rico\n",
    "    if y.dtype == 'object' or not pd.api.types.is_numeric_dtype(y):\n",
    "        print(\"üîÑ Convirtiendo target a num√©rico...\")\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "        print(f\"‚úÖ Target codificado - clases: {le.classes_}\")\n",
    "    \n",
    "    # 4. LIMPIAR Y VALIDAR FEATURES\n",
    "    print(\"üîç Validando features...\")\n",
    "    \n",
    "    # Eliminar columnas con todos valores nulos\n",
    "    null_cols = X.columns[X.isnull().all()]\n",
    "    if len(null_cols) > 0:\n",
    "        print(f\"üóëÔ∏è Eliminando {len(null_cols)} columnas con todos valores nulos\")\n",
    "        X = X.drop(columns=null_cols)\n",
    "        feature_cols = [col for col in feature_cols if col not in null_cols]\n",
    "    \n",
    "    # Llenar valores nulos en features num√©ricas\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "    \n",
    "    # Convertir features categ√≥ricas a num√©rico\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"üî§ Convirtiendo {len(categorical_cols)} columnas categ√≥ricas...\")\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Asegurar que todas las features sean num√©ricas\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    X = X.fillna(0)  # Llenar cualquier NaN resultante\n",
    "    \n",
    "    print(f\"‚úÖ Datos limpiados - X: {X.shape}, y: {y.shape}\")\n",
    "    \n",
    "    # 5. VERIFICAR DISTRIBUCI√ìN DEL TARGET\n",
    "    target_distribution = y.value_counts().to_dict()\n",
    "    print(f\"üìà Distribuci√≥n del target: {target_distribution}\")\n",
    "    \n",
    "    # Calcular balance de clases DE FORMA SEGURA\n",
    "    target_counts = y.value_counts()\n",
    "    minority_class_ratio = target_counts.min() / target_counts.sum()\n",
    "    print(f\"‚öñÔ∏è Ratio clase minoritaria: {minority_class_ratio:.3f}\")\n",
    "    \n",
    "    if minority_class_ratio < 0.1:\n",
    "        print(\"‚ö†Ô∏è Dataset muy desbalanceado - usando class_weight='balanced'\")\n",
    "        class_weight = 'balanced'\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset relativamente balanceado\")\n",
    "        class_weight = None\n",
    "    \n",
    "    # 6. SPLIT TRAIN/TEST\n",
    "    print(\"\\nüîÑ Dividiendo datos en train/test...\")\n",
    "    \n",
    "    test_size = 0.2 if len(df) > 5000 else 0.3\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=test_size, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "        print(f\"‚úÖ Split estratificado exitoso\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ö†Ô∏è Error en split estratificado: {e}\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        print(f\"‚úÖ Split sin estratificar completado\")\n",
    "    \n",
    "    print(f\"üìä Train set: {X_train.shape[0]} muestras\")\n",
    "    print(f\"üìä Test set: {X_test.shape} muestras\")\n",
    "    \n",
    "    # 7. MODELO BASELINE\n",
    "    print(\"\\n‚ö° Entrenando modelo baseline...\")\n",
    "    \n",
    "    rf_baseline = RandomForestClassifier(\n",
    "        n_estimators=50,  # Reducido para velocidad\n",
    "        random_state=42, \n",
    "        n_jobs=-1,\n",
    "        class_weight=class_weight,\n",
    "        max_depth=10  # Limitado para evitar overfitting\n",
    "    )\n",
    "    \n",
    "    rf_baseline.fit(X_train, y_train)\n",
    "    baseline_score = rf_baseline.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"üìä Accuracy modelo baseline: {baseline_score:.4f}\")\n",
    "    \n",
    "    # 8. OPTIMIZACI√ìN SIMPLIFICADA (para evitar errores)\n",
    "    print(\"\\nüîß Optimizaci√≥n simplificada de hiperpar√°metros...\")\n",
    "    \n",
    "    # Grid m√°s peque√±o para evitar problemas\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [10, 15, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    print(f\"üîç Probando {np.prod([len(v) for v in param_grid.values()])} combinaciones...\")\n",
    "    \n",
    "    try:\n",
    "        grid_search = GridSearchCV(\n",
    "            RandomForestClassifier(\n",
    "                random_state=42, \n",
    "                n_jobs=-1,\n",
    "                class_weight=class_weight\n",
    "            ),\n",
    "            param_grid,\n",
    "            cv=3,  # Reducido de 5 a 3 para velocidad\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0  # Reducir verbosidad\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_rf = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"‚úÖ Optimizaci√≥n completada\")\n",
    "        print(f\"üìä Mejores par√°metros:\")\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en optimizaci√≥n: {e}\")\n",
    "        print(\"üîÑ Usando modelo baseline...\")\n",
    "        best_rf = rf_baseline\n",
    "    \n",
    "    # 9. PREDICCIONES Y M√âTRICAS\n",
    "    print(\"\\nüéØ Realizando predicciones...\")\n",
    "    \n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    \n",
    "    # Probabilidades (solo para clasificaci√≥n binaria)\n",
    "    y_pred_proba = None\n",
    "    if len(best_rf.classes_) == 2:\n",
    "        try:\n",
    "            y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "        except:\n",
    "            y_pred_proba = None\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc_auc = None\n",
    "    if len(best_rf.classes_) == 2 and y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        except:\n",
    "            roc_auc = None\n",
    "    \n",
    "    # Cross-validation\n",
    "    try:\n",
    "        cv_scores = cross_val_score(best_rf, X_train, y_train, cv=3, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "    except:\n",
    "        cv_mean = accuracy\n",
    "        cv_std = 0.0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n === RESULTADOS FINALES ===\")\n",
    "    print(f\" Accuracy: {accuracy:.4f}\")\n",
    "    print(f\" Precision: {precision:.4f}\")\n",
    "    print(f\" Recall: {recall:.4f}\")\n",
    "    print(f\" F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\" ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\" CV Score: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    \n",
    "    print(f\"\\n Matriz de Confusi√≥n:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # 10. FEATURE IMPORTANCE\n",
    "    print(\"\\nüîù === FEATURE IMPORTANCE ===\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,  # Usar X.columns en lugar de feature_cols\n",
    "        'importance': best_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Features m√°s importantes:\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<40} {row['importance']:.4f}\")\n",
    "    \n",
    "    # 11. PREPARAR M√âTRICAS PARA RETORNO\n",
    "    detailed_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'baseline_accuracy': baseline_score,\n",
    "        'improvement': accuracy - baseline_score,\n",
    "        'n_features': len(X.columns),\n",
    "        'n_samples_train': len(X_train),\n",
    "        'n_samples_test': len(X_test),\n",
    "        'class_distribution': target_distribution\n",
    "    }\n",
    "    \n",
    "    improvement = detailed_metrics['improvement']\n",
    "    print(f\"\\nüöÄ Mejora vs baseline: {improvement:+.4f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"‚úÖ Optimizaci√≥n exitosa - modelo mejorado\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Modelo optimizado igual o peor que baseline\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ === ENTRENAMIENTO COMPLETADO ===\")\n",
    "    \n",
    "    return best_rf, X_test, y_test, y_pred, y_pred_proba, feature_importance, detailed_metrics\n",
    "\n",
    "# === EJECUTAR CON VALIDACI√ìN ROBUSTA ===\n",
    "\n",
    "print(\"üîß === CONFIGURACI√ìN INICIAL ===\")\n",
    "\n",
    "target_column = \"PerdidaCartera\"\n",
    "\n",
    "# Verificar que el target existe\n",
    "if target_column not in df_processed.columns:\n",
    "    print(f\"ERROR: Target '{target_column}' no encontrado\")\n",
    "    print(f\"Columnas disponibles: {list(df_processed.columns)}\")\n",
    "else:\n",
    "    print(f\"Target '{target_column}' encontrado\")\n",
    "\n",
    "    # Preparar features\n",
    "    excluded_columns = [\n",
    "        target_column,\n",
    "        'PersonaCreditoCodigo', \n",
    "        'IdentificacionCliente', \n",
    "        'TipoIdentificacion', \n",
    "        'CorreoElectronicoCliente', \n",
    "        'LocalCreditMasterIdSistecredito'\n",
    "    ]\n",
    "    \n",
    "    feature_columns = [col for col in df_processed.columns if col not in excluded_columns]\n",
    "    \n",
    "    print(f\"Target: {target_column}\")\n",
    "    print(f\"Features seleccionadas: {len(feature_columns)}\")\n",
    "    print(f\"Columnas excluidas: {len(excluded_columns)}\")\n",
    "    \n",
    "    if len(feature_columns) == 0:\n",
    "        print(\"ERROR: No hay features v√°lidas para entrenar\")\n",
    "    else:\n",
    "        print(f\"Listo para entrenar con {len(feature_columns)} features\")\n",
    "        \n",
    "        # ENTRENAR MODELO\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INICIANDO ENTRENAMIENTO DEL MODELO\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            model, X_test, y_test, y_pred, y_pred_proba, feature_importance, detailed_metrics = train_random_forest_model(\n",
    "                df_processed, \n",
    "                target_column, \n",
    "                feature_columns\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Mostrar resumen final\n",
    "            print(f\"\\n=== RESUMEN FINAL ===\")\n",
    "            print(f\" Modelo: Random Forest Classifier\")\n",
    "            print(f\"Target: {target_column}\")\n",
    "            print(f\"Features: {detailed_metrics['n_features']}\")\n",
    "            print(f\"Train samples: {detailed_metrics['n_samples_train']}\")\n",
    "            print(f\"Test samples: {detailed_metrics['n_samples_test']}\")\n",
    "            print(f\"Final Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Cross-Val Score: {detailed_metrics['cv_mean']:.4f}\")\n",
    "            print(f\"Feature m√°s importante: {feature_importance.iloc[0]['feature']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR durante entrenamiento: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17173458-21e0-4cc9-b807-5323abd63f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_model_to_adls_only(model, manifest, encoders, storage_account_name):\n",
    "    \"\"\"Guardar modelo, manifest y encoders SOLO en ADLS Gen2 - SIN repositorio\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import tempfile\n",
    "    import joblib\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    print(\"üíæ === GUARDANDO MODELO SOLO EN ADLS GEN2 ===\")\n",
    "    \n",
    "    # Configurar cliente ADLS Gen2\n",
    "    try:\n",
    "        from azure.storage.filedatalake import DataLakeServiceClient\n",
    "        \n",
    "        service_client = DataLakeServiceClient(\n",
    "            account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
    "            credential=os.getenv('AZURE_STORAGE_KEY')\n",
    "        )\n",
    "        \n",
    "        container_name = \"sistecredito2\"\n",
    "        file_system_client = service_client.get_file_system_client(container_name)\n",
    "        \n",
    "        print(\"‚úÖ Cliente ADLS Gen2 configurado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error configurando ADLS Gen2: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Crear timestamp para carpeta √∫nica\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_folder = f\"models/random_forest_perdida_cartera_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # 1. GUARDAR MODELO\n",
    "        print(\"Guardando modelo...\")\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.joblib') as tmp_file:\n",
    "            joblib.dump(model, tmp_file.name)\n",
    "            \n",
    "            with open(tmp_file.name, 'rb') as f:\n",
    "                model_data = f.read()\n",
    "            \n",
    "            file_client = file_system_client.get_file_client(f\"{model_folder}/model.joblib\")\n",
    "            file_client.upload_data(model_data, overwrite=True)\n",
    "            \n",
    "        os.unlink(tmp_file.name)\n",
    "        print(f\"Modelo guardado: {model_folder}/model.joblib\")\n",
    "        \n",
    "        # 2. GUARDAR MANIFEST\n",
    "        print(\"üìã Guardando manifest...\")\n",
    "        \n",
    "        manifest_json = json.dumps(manifest, indent=2, ensure_ascii=False)\n",
    "        manifest_bytes = manifest_json.encode('utf-8')\n",
    "        \n",
    "        file_client = file_system_client.get_file_client(f\"{model_folder}/manifest.json\")\n",
    "        file_client.upload_data(manifest_bytes, overwrite=True)\n",
    "        \n",
    "        print(f\"‚úÖ Manifest guardado: {model_folder}/manifest.json\")\n",
    "        \n",
    "        # 3. GUARDAR ENCODERS (SI EXISTEN)\n",
    "        if encoders and len(encoders) > 0:\n",
    "            print(\"üî§ Guardando encoders...\")\n",
    "            \n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.joblib') as tmp_file:\n",
    "                joblib.dump(encoders, tmp_file.name)\n",
    "                \n",
    "                with open(tmp_file.name, 'rb') as f:\n",
    "                    encoders_data = f.read()\n",
    "                \n",
    "                file_client = file_system_client.get_file_client(f\"{model_folder}/encoders.joblib\")\n",
    "                file_client.upload_data(encoders_data, overwrite=True)\n",
    "                \n",
    "            os.unlink(tmp_file.name)\n",
    "            print(f\"‚úÖ Encoders guardados: {model_folder}/encoders.joblib\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No hay encoders para guardar\")\n",
    "        \n",
    "        # 4. CREAR README INFORMATIVO\n",
    "        print(\"üìÑ Creando README...\")\n",
    "        \n",
    "        readme_content = f\"\"\"# Modelo Random Forest - Perdida de Cartera\n",
    "\n",
    "## Informaci√≥n del Modelo\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Fecha**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Tipo**: Random Forest Classifier\n",
    "- **Target**: PerdidaCartera\n",
    "- **Accuracy**: {manifest['model_performance']['accuracy']:.4f}\n",
    "\n",
    "## Archivos en esta Carpeta\n",
    "- `model.joblib` - Modelo entrenado serializado\n",
    "- `manifest.json` - Metadatos completos del modelo\n",
    "- `encoders.joblib` - Encoders para preprocesamiento (si aplica)\n",
    "- `README.md` - Este archivo\n",
    "\n",
    "## Top 5 Features M√°s Importantes\n",
    "{chr(10).join([f\"- {feat['feature']}: {feat['importance']:.4f}\" for feat in manifest['feature_importance']['top_10_features'][:5]])}\n",
    "\n",
    "## Performance\n",
    "- Accuracy: {manifest['model_performance']['accuracy']:.4f}\n",
    "- Precision: {manifest['model_performance']['precision']:.4f}\n",
    "- Recall: {manifest['model_performance']['recall']:.4f}\n",
    "- F1-Score: {manifest['model_performance']['f1_score']:.4f}\n",
    "- Cross-Validation: {manifest['model_performance']['cross_validation_mean']:.4f} ¬± {manifest['model_performance']['cross_validation_std']:.4f}\n",
    "\n",
    "## Uso del Modelo\n",
    "\n",
    "---\n",
    "*Generado autom√°ticamente por Pipeline MLOps*\n",
    "\"\"\"\n",
    "        \n",
    "        readme_bytes = readme_content.encode('utf-8')\n",
    "        file_client = file_system_client.get_file_client(f\"{model_folder}/README.md\")\n",
    "        file_client.upload_data(readme_bytes, overwrite=True)\n",
    "        \n",
    "        print(f\"README creado: {model_folder}/README.md\")\n",
    "        \n",
    "        # 5. VERIFICAR ARCHIVOS GUARDADOS\n",
    "        print(\"\\n=== ARCHIVOS GUARDADOS EN ADLS GEN2 ===\")\n",
    "        \n",
    "        try:\n",
    "            files = file_system_client.get_paths(path=model_folder)\n",
    "            for file_path in files:\n",
    "                if not file_path.is_directory:\n",
    "                    file_size = file_path.content_length or 0\n",
    "                    print(f\"  {file_path.name} ({file_size:,} bytes)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No se pudo listar archivos: {e}\")\n",
    "        \n",
    "        # 6. INFORMACI√ìN FINAL\n",
    "        full_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{model_folder}/\"\n",
    "        \n",
    "        print(f\"\\n=== GUARDADO COMPLETADO EXITOSAMENTE ===\")\n",
    "        print(f\"Ubicaci√≥n: {full_path}\")\n",
    "        print(f\"Timestamp: {timestamp}\")\n",
    "        print(f\"Accuracy del modelo: {manifest['model_performance']['accuracy']:.4f}\")\n",
    "        print(f\"Feature m√°s importante: {manifest['feature_importance']['top_10_features'][0]['feature']}\")\n",
    "        \n",
    "        return {\n",
    "            'adls_path': full_path,\n",
    "            'folder_name': model_folder,\n",
    "            'timestamp': timestamp,\n",
    "            'files_saved': ['model.joblib', 'manifest.json', 'encoders.joblib' if encoders else None, 'README.md']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando en ADLS Gen2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_model_manifest_for_perdida_cartera(model, df, feature_cols, target_col, metrics, feature_importance, encoders=None):\n",
    "    \"\"\"Crear manifest espec√≠fico para modelo de PerdidaCartera\"\"\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    manifest = {\n",
    "        \"model_info\": {\n",
    "            \"model_type\": \"RandomForestClassifier\",\n",
    "            \"algorithm\": \"Random Forest\",\n",
    "            \"created_date\": datetime.now().isoformat(),\n",
    "            \"sklearn_version\": \"1.0+\",\n",
    "            \"model_parameters\": model.get_params(),\n",
    "            \"use_case\": \"Predicci√≥n de P√©rdida de Cartera\",\n",
    "            \"business_problem\": \"Clasificar clientes seg√∫n riesgo de p√©rdida en cartera\"\n",
    "        },\n",
    "        \"data_info\": {\n",
    "            \"dataset_shape\": df.shape,\n",
    "            \"total_features\": len(feature_cols),\n",
    "            \"target_column\": target_col,\n",
    "            \"feature_columns\": feature_cols,\n",
    "            \"categorical_features\": list(encoders.keys()) if encoders else [],\n",
    "            \"data_source\": \"Sistema Sistecredito - Datos de Cartera\",\n",
    "            \"excluded_columns\": [\n",
    "                \"PersonaCreditoCodigo\", \n",
    "                \"IdentificacionCliente\", \n",
    "                \"TipoIdentificacion\", \n",
    "                \"CorreoElectronicoCliente\", \n",
    "                \"LocalCreditMasterIdSistecredito\"\n",
    "            ]\n",
    "        },\n",
    "        \"model_performance\": {\n",
    "            \"accuracy\": float(metrics['accuracy']),\n",
    "            \"precision\": float(metrics['precision']),\n",
    "            \"recall\": float(metrics['recall']),\n",
    "            \"f1_score\": float(metrics['f1_score']),\n",
    "            \"roc_auc\": float(metrics['roc_auc']) if metrics.get('roc_auc') else None,\n",
    "            \"cross_validation_mean\": float(metrics['cv_mean']),\n",
    "            \"cross_validation_std\": float(metrics['cv_std']),\n",
    "            \"confusion_matrix\": metrics['confusion_matrix'],\n",
    "            \"baseline_accuracy\": float(metrics.get('baseline_accuracy', 0)),\n",
    "            \"improvement_vs_baseline\": float(metrics.get('improvement', 0))\n",
    "        },\n",
    "        \"feature_importance\": {\n",
    "            \"top_10_features\": feature_importance.head(10).to_dict('records'),\n",
    "            \"all_features\": feature_importance.to_dict('records')\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"label_encoders\": {k: v.classes_.tolist() if hasattr(v, 'classes_') else str(v) \n",
    "                              for k, v in encoders.items()} if encoders else {},\n",
    "            \"missing_values_treatment\": \"median_fill_numeric_zero_fill_categorical\",\n",
    "            \"duplicates_removed\": True,\n",
    "            \"null_columns_removed\": True\n",
    "        },\n",
    "        \"model_usage\": {\n",
    "            \"prediction_example\": \"model.predict(X_new)\",\n",
    "            \"probability_example\": \"model.predict_proba(X_new)\",\n",
    "            \"required_features\": feature_cols,\n",
    "            \"input_validation\": \"Ensure all features are numeric, no nulls allowed\"\n",
    "        },\n",
    "        \"business_metrics\": {\n",
    "            \"target_distribution\": metrics.get('class_distribution', {}),\n",
    "            \"training_samples\": metrics.get('n_samples_train', 0),\n",
    "            \"test_samples\": metrics.get('n_samples_test', 0),\n",
    "            \"class_balance_ratio\": \"balanced\" if metrics.get('class_distribution', {}) else \"unknown\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "# === C√ìDIGO COMPLETO PARA USAR DESPU√âS DEL ENTRENAMIENTO ===\n",
    "\n",
    "# Despu√©s de entrenar tu modelo, ejecuta esto:\n",
    "\n",
    "print(\"\\n=== GUARDANDO MODELO EN ADLS GEN2 ===\")\n",
    "\n",
    "# Crear manifest completo\n",
    "manifest = create_model_manifest_for_perdida_cartera(\n",
    "    model, \n",
    "    df_processed, \n",
    "    feature_importance['feature'].tolist(),  # Lista de features del DataFrame\n",
    "    target_column, \n",
    "    detailed_metrics, \n",
    "    feature_importance,\n",
    "    encoders if 'encoders' in locals() else None\n",
    ")\n",
    "\n",
    "# Guardar SOLO en ADLS Gen2\n",
    "result = save_model_to_adls_only(\n",
    "    model, \n",
    "    manifest, \n",
    "    encoders if 'encoders' in locals() else None,\n",
    "    \"sistecreditofinal\"\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n === GUARDADO EXITOSO ===\")\n",
    "    print(f\"Ubicaci√≥n: {result['adls_path']}\")\n",
    "    print(f\"Archivos: {', '.join([f for f in result['files_saved'] if f])}\")\n",
    "    print(f\"ID del modelo: {result['timestamp']}\")\n",
    "    \n",
    "    # Copiar la ruta para uso futuro\n",
    "    model_path_for_tests = result['folder_name']\n",
    "    print(f\"\\nPara tests CI/CD usa esta ruta:\")\n",
    "    print(f\"model_path = '{model_path_for_tests}'\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error guardando modelo\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "entrenamiento",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
